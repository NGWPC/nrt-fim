{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Demo: Compare CNN output to benchmark FIM using autoeval process\n",
    "\n",
    "Prerequisite: Repositories `autoeval-jobs`, `f1-trainer`, and input data.\n",
    "\n",
    "To run locally:\n",
    "1. `mkdir /home/pi_7/subcase_4_CNN`\n",
    "2. Clone `f1-trainer` and `autoeval-jobs` to `/home/pi_7/subcase_4_CNN`\n",
    "3. `mkdir autoeval-jobs/agreement_maker/data` and `mkdir f1-trainer/examples/data`\n",
    "4. Place desired comparison in `f1/examples/data` as `model_domain.gpkg`, `cnn_prediction.tif`, and `benchmark.tif`\n",
    "5. In `f1-trainer1`, `uv venv .venv` -> `source .venv/bin/activate` -> `uv pip install -r pyproject.toml --extra jupyter`\n",
    "6. Run notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import rasterio.mask\n",
    "import xarray as xr\n",
    "from rasterio.enums import Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run from pi_7/subcase_4_CNN\n",
    "os.chdir(\"../../\")\n",
    "print(f\"Working dir changed to {os.getcwd()}\")\n",
    "os.makedirs(\"./f1-trainer/examples/data\", exist_ok=True)\n",
    "os.makedirs(\"./autoeval-jobs/agreement_maker/data\", exist_ok=True)\n",
    "f1_data_path = Path(\"./f1-trainer/examples/data\")\n",
    "eval_data_path = Path(\"./autoeval-jobs/agreement_maker/data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_fim(\n",
    "    raster: str | Path,\n",
    "    fim_extent: str | Path,\n",
    "    output: str | Path,\n",
    "    threshold: int,\n",
    "    input_nodata: int = 1e20,\n",
    "    output_nodata: int = 255,\n",
    "):\n",
    "    \"\"\"Generate a FIM autoeval-compatible raster from CNN output\n",
    "\n",
    "    Classify wet (1) where >= threshold\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raster : str\n",
    "        input path\n",
    "    fim_extent : str\n",
    "        fim model domain path (vector)\n",
    "    output : str\n",
    "        output path\n",
    "    threshold : int\n",
    "        classify wet (1) where >= threshold else dry (0)\n",
    "    input_nodata : int, optional\n",
    "        no data value in input, by default 1e20\n",
    "    output_nodata : int, optional\n",
    "        no data value for output, by default 255\n",
    "    \"\"\"\n",
    "    with rasterio.open(raster, \"r\") as src:\n",
    "        profile = src.profile\n",
    "\n",
    "    print(\"Reading model extent\")\n",
    "    gdf = gpd.read_file(fim_extent, layer=\"model_domain\")\n",
    "    gdf = gdf.to_crs(profile[\"crs\"])\n",
    "\n",
    "    print(\"Clipping raster to model extent\")\n",
    "    with rasterio.open(raster, \"r\") as src:\n",
    "        data, transform = rasterio.mask.mask(src, gdf.geometry.tolist(), all_touched=True, crop=True)\n",
    "\n",
    "    print(\"Re-classifying raster to binary\")\n",
    "    data[data == input_nodata] = output_nodata\n",
    "    data[(data < threshold) & (data != output_nodata)] = 0\n",
    "    data[(data >= threshold) & (data != output_nodata)] = 1\n",
    "\n",
    "    print(f\"Writing output to {output}\")\n",
    "    profile.update(\n",
    "        transform=transform,\n",
    "        height=data.shape[1],\n",
    "        width=data.shape[2],\n",
    "        nodata=255,\n",
    "        dtype=rasterio.uint8,\n",
    "        tiled=\"YES\",\n",
    "        compress=\"deflate\",\n",
    "        blockxsize=512,\n",
    "        blockysize=512,\n",
    "    )\n",
    "    with rasterio.open(output, \"w\", **profile) as dst:\n",
    "        dst.write(data)\n",
    "\n",
    "\n",
    "def resample_benchmark_fim(\n",
    "    benchmark: str | Path, output_path: str | Path, target_resolution: int | float = 250\n",
    ") -> None:\n",
    "    \"\"\"Helper to resample benchmark FIMs\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    benchmark : str\n",
    "        path to FIM benchmark\n",
    "    target_resolution : int | float\n",
    "        output pixel resolution\n",
    "    output_path : str\n",
    "        output path\n",
    "    \"\"\"\n",
    "    raster = xr.open_dataset(benchmark, engine=\"rasterio\", chunked_array_type=\"cubed\")\n",
    "\n",
    "    raster = raster.rio.reproject(\n",
    "        raster.rio.crs, resolution=target_resolution, resampling=Resampling.bilinear\n",
    "    )\n",
    "\n",
    "    raster[\"band_data\"].rio.to_raster(\n",
    "        output_path, tiled=True, compression=\"deflate\", blockxsize=512, blockysize=512\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Generate a binary CNN FIM in benchmark FIM model domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fim_extent = f1_data_path / \"model_domain.gpkg\"\n",
    "cnn = f1_data_path / \"cnn_prediction.tif\"\n",
    "cnn_output = eval_data_path / \"test_cnn.tif\"\n",
    "\n",
    "generate_eval_fim(\n",
    "    raster=cnn,\n",
    "    threshold=45,\n",
    "    fim_extent=fim_extent,\n",
    "    output=cnn_output,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Resample Benchmark FIM to match CNN output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may take 5+ minutes\n",
    "fim_benchmark = f1_data_path / \"benchmark.tif\"\n",
    "benchmark_output = eval_data_path / \"test_benchmark.tif\"\n",
    "\n",
    "resample_benchmark_fim(\n",
    "    benchmark=fim_benchmark,\n",
    "    output_path=benchmark_output,\n",
    "    target_resolution=250,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Run auto-eval make agreement process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ~\n",
    "cd pi_7/subcase_4_CNN/autoeval-jobs\n",
    "docker compose build\n",
    "docker compose up -d\n",
    "docker compose exec make-agreement-dev bash\n",
    "python make_agreement.py --fim_type extent --candidate_path /app/data/test_cnn.tif --benchmark_path /app/data/test_benchmark.tif --output_path /app/data/eval_output.tif --metrics_path /app/data/cnn_output_metrics.csv\n",
    "chmod  777 ./data/eval_output.tif\n",
    "exit\n",
    "docker compose down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Display metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(eval_data_path / \"cnn_output_metrics.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Display maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(eval_data_path / \"eval_output.tif\", \"r\") as src:\n",
    "    eval = src.read(1)\n",
    "    eval = np.where(eval == 255, np.nan, eval)\n",
    "\n",
    "with rasterio.open(cnn_output, \"r\") as src:\n",
    "    cnn = src.read(1)\n",
    "    cnn = np.where(cnn == 255, np.nan, cnn)\n",
    "\n",
    "with rasterio.open(benchmark_output, \"r\") as src:\n",
    "    benchmark = src.read(1)\n",
    "    benchmark = np.where(benchmark == 255, np.nan, benchmark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "f, ax = plt.subplots(1, 3, figsize=(14, 5))\n",
    "ax[0].imshow(benchmark, cmap=\"Blues\")\n",
    "ax[1].imshow(cnn, cmap=\"Blues\")\n",
    "ax[2].imshow(eval, cmap=\"viridis\")\n",
    "ax[0].set_axis_off()\n",
    "ax[1].set_axis_off()\n",
    "ax[2].set_axis_off()\n",
    "\n",
    "# legend for benchmark and CNN\n",
    "cmap = plt.get_cmap(\"Blues\")\n",
    "rgba = [cmap(0), cmap(0.99)]\n",
    "legend_elements = [\n",
    "    Patch(facecolor=rgba[1], edgecolor=\"black\", label=\"Wet\"),\n",
    "    Patch(facecolor=rgba[0], edgecolor=\"black\", label=\"Dry\"),\n",
    "]\n",
    "ax[0].legend(title=\"Benchmark\", handles=legend_elements, loc=\"lower left\")\n",
    "ax[1].legend(title=\"CNN\", handles=legend_elements, loc=\"lower left\")\n",
    "\n",
    "# legend for eval\n",
    "cmap = plt.get_cmap(\"viridis\")\n",
    "rgba = [cmap(0), cmap(0.33), cmap(0.66), cmap(0.99)]\n",
    "legend_elements = [\n",
    "    Patch(facecolor=rgba[0], edgecolor=\"black\", label=\"True Negative\"),\n",
    "    Patch(facecolor=rgba[1], edgecolor=\"black\", label=\"False Negative\"),\n",
    "    Patch(facecolor=rgba[2], edgecolor=\"black\", label=\"False Positive\"),\n",
    "    Patch(facecolor=rgba[3], edgecolor=\"black\", label=\"True Positive\"),\n",
    "]\n",
    "ax[2].legend(title=\"Eval\", handles=legend_elements, loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1-trainer (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
